* Setting up a Dev Environment

When working on a python project, a common way that people will manage their dependencies is running =pip freeze > requirements.txt= and coupling that with =virtualenv= to manage project level dependencies.

Often, when reproducing somebody else's analysis, it is not enough to run =pip install -r requirements.txt= in the repository.  

However, sometimes there are configurations that are specific system level dependencies that are not captured. As I go along with developing my code, I will install system-level dependencies as required by the package.  

To be able to replicate all the system level dependencies, you can see how Docker could easily be used! 

Now we move to the next stage, which is figuring out how to layer instructions, and put together a Dockerfile to set up a docker container with your exact specifications to replicate your results. 

In the past few weeks, we have observed the following issues: 
1. Somebody builds a tool in a different flavour of python with several package changes 
2. Having pandas 0.20 on one machine and pandas 0.21 on another can change how the metrics are calculated slightly
3. Struggling to manage =virtualenvs= for different packages, broken =virtualenvs=

Here, I will spin up a Python container, with all the tools/packages that are required for my project.
This includes setting up a Jupyter notebook server with access to all those packages.

I can open a Jupyter notebook in this container, and I am able to import all the packages that I need inside this Jupyter notebook.


The use case we will explore here, is setting up a python machine with all the tools required for a project, and then setting up a jupyter notebook server within to access all those resources: 

In the terminal run:
#+BEGIN_EXAMPLE
docker build -t devenv .
#+END_EXAMPLE

Build the image using the dockerfile. Tag it "devenv"
The dot means setting the local machine directory to the same directory where the dockerfile is located (and where all the files your docker
file is calling is located).

This should set up a jupyter notebook that you will be able to access with all the tools in the =requirements.txt= file installed. 
Here, I use docker run devenv to create a new container from the devnev image and start it up, and then run any commands specified in the docker file.

I am mapping the port 8888 in the container to port 8888 in my local machine. So in my local machine, I can open localhost:8888, which in turn will be used to access port 8888 in the container (that's the port of the Jupyter notebook server)


#+BEGIN_EXAMPLE
docker run -p 8888:8888 devenv
#+END_EXAMPLE

Now access it from your machine, try =localhost:8888=. It will ask you to copy and paste the token you were given.
OR, you can just copy and paste the url that is given in the terminal: http://127.0.0.1:8888/?token=66a6db59ad53349417cf50dfa72bf9e4eb1e32566d707534

If you run this url on your browser, you will have access to the Jupyter notebook that is running in the container. It has all the Python packages that is specified in the requirements.txt installed. 

The container is now running on your machine. And you have a notebook running on that container, which can be accessed via the localhost port 8888 on your machine.

Remember that the container is ephemeral (temp). If I shutdown the kernel, the notebook will be gone, as well as the container.
Use docker ps to check for any running container (there will be none)
Use docker ps -a to check for any previous running container (I will see a record for devenv, which tells me when it was created, when it was exited, and what commands I ran in the docker (jupyter notebook)
If I want to share my jupyter notebook with my teammate, I can copy that notebook (ipynb) into my container, and then I will set up the container with all dependencies, and start up jupyter when the container is created and run (so my teammate can open the jupyter-notebook that I shared). If I use docker this way, I wouldn't have anymore worries about different versions of Python/packages being installed, and any system/OS level dependencies that are different between me and my teammate.

Another way (say I want to run a Python script on a container) is to mount a volumne to the container. What this means is that I will select a folder on my local machine, copy my .py script in it, and mount that folder on the container. Anything I do in the folder in my local machine is "sync" to the container. If I make any changes to the .py script, those changes will automatically will "transfer over" to the script in the container.
